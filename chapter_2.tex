%   Filename    : chapter_2.tex 
\chapter{Review of Related Literature}
\label{sec:relatedlit}

\section{Communication Gap between Generations}
Language is dynamic in nature thus, constantly evolving over time. One example of this behavior is the development of internet slang. Internet slang is a result of language variation and is often regarded as informal \cite{Liu_Gui_Zuo_Dai_2019}. In the study, \textit{The Use of Online Slang for Independent Learning in English Vocabulary} \cite{Ambarsari_Amrullah_Nawawi_2020}, students used internet slang to express their feelings and emotions, and to align their communication style with their peers. 

However, this development has its challenges. It is suggested that younger generation should use slang to communicate with each instead of older generations because it might cause confusion between them \cite{Jeresano_Carretero_2022}.

This miscommunication is prominent between generations with differences in lingiuistic familiarity as Suslak \cite{SUSLAK2009199} argues that age influences language use, noting that language evolves across generations.
Supporting this, a study by Teng and Joo \cite{Teng_Joo2023} found that the older a person is, the less likely they are to understand internet language.

Th studies showed that using internet slang improves relationship between those who use it. However, using internet slang for intergenrational communication may result in miscommunication.


\section{Existing Studies}
Khazeni et al. used deep learning to create a model for translating Persian slang text into formal ones \cite{Khazeni}.
They were able to create a model to convert texts from social media into sentiments for classification.
Nocon et al. \cite{Nocon_Kho_Arroyo_2018} created a Filipino colloquialism translator using Tensorflow’s sequence-to-sequence model and Moses’ phrase-based statistical machine translation.
They found that the Moses model was able to create a natural sounding translation, while the Tensorflow model often produced bad sentences.

A slang translation system developed by Ibrahim and Mustafa \cite{Abdulstar_Ibrahim_Shareef_Mustafa_2023} used models obtained from Hugging Face, a repository of pre-trained models, and retrained it using a dataset containing slang and their corresponding definition and example.
They determined that these models can be tweaked into learning the relationship between the slang and its meaning.

\section{LoRA for Fine Tuning}
Low Rank Adaptation, or LoRA, is an efficient Parameter Efficient Fine Tuning (PEFT) method proposed by Hu et al \cite{hu2021loralowrankadaptationlarge}.
It can significantly decrease the required storage for training while producing comparable results and in some cases, even outperforming other adaptation methods.
In addition, it has minimal chance of catastrophic forgetting as the original weights are not being tampered with, unlike other finetuning methods.
These factors make it a suitable option for slang translation as a quick yet accurate solution.
In a study conducted by Zhao et al. \cite{zhao2024loraland310finetuned}, they determined that some LLMs using LoRA for fine tuning can outperform GPT-4, one of the most advanced LLM models currently.
A study by Nguyen et al. \cite{nguyen2023finetuningllama2large} used LoRA in fine tuning a pre-trained Llama 2 7B model for text classification of a dataset that contains slang.
They were able to create a more accurate model compared to models by existing studies at that time. 

\section{Chapter Summary}
This chapter shows how generational differences create communication gaps, especially due to internet slang.
Younger people tend to use slang to express emotions and connect with friends, but this can confuse older generations who aren't as familiar with these terms.
Research shows that as language changes over time, older people are generally less likely to understand the newest internet language.
To bridge this gap, some recent studies have utilized machine learning to translate slang into more standard language.
For instance, Khazeni et al. \cite{Khazeni} used deep learning to translate Persian slang, while Nocon et al. \cite{Nocon_Kho_Arroyo_2018} created a Filipino slang translator using statistical models.
Moreover, Ibrahim and Mustafa \cite{Abdulstar_Ibrahim_Shareef_Mustafa_2023} fine-tuned pre-trained models to learn slang meanings.
One of the promising techniques for this is Low Rank Adaptation (LoRA), which is a fine-tuning method that keeps the original model stable while using less storage.
Studies by Zhao et al. \cite{zhao2024loraland310finetuned} and Nguyen et al. \cite{nguyen2023finetuningllama2large} show that LoRA models are not only efficient but can even outperform advanced models like GPT-4 when it comes to slang translation and text classification. 