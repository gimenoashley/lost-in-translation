%   Filename    : chapter_2.tex 
\chapter{Review of Related Literature}
\label{sec:relatedlit}

\section{Communication Gap between Generations}
Language is dynamic in nature thus, constantly evolving over time. One example of this behavior is the development of internet slang. Internet slang is a result of language variation and is often regarded as informal \cite{Liu_Gui_Zuo_Dai_2019}. In the study, \textit{The Use of Online Slang for Independent Learning in English Vocabulary} \cite{Ambarsari_Amrullah_Nawawi_2020}, students used internet slang to express their feelings and emotions, and to align their communication style with their peers. 

However, this development has its challenges. It is suggested that younger generation should use slang to communicate with each instead of older generations because it might cause confusion between them \cite{Jeresano_Carretero_2022}.

This miscommunication is prominent between generations with differences in lingiuistic familiarity as Suslak \cite{SUSLAK2009199} argues that age influences language use, noting that language evolves across generations.
Supporting this, a study by Teng and Joo \cite{Teng_Joo2023} found that the older a person is, the less likely they are to understand internet language.

The studies showed that using internet slang improves relationship between those who use it. However, using internet slang for inter-generational communication can be a hindrance to proper and effective communication \cite{gonzagaforda}.


\section{Existing Studies}
Khazeni et al. \cite{Khazeni} used deep learning to create a model for translating Persian slang text into formal ones. The researchers explored the challenges of translating Persian slang into English within the context of film subtitling, specifically focusing on the performance of three neural machine translation (NMT) systems, namely Google Translate, Targoman, and Farazin. The primary interest of the paper lies in the understanding how these NMTs systems handle  the complexities of slang translation. It was revealed that the NMT systems often struggle to capture the nuances of slang, leading to unnatural and inaccurate translations. Targoman performed best in naturalness, it fell short of human translation quality. This implied the need for specialized algorithms or training data suitable for slang, and potentially human post-editing, to achieve accurate and culturally appropriate translations in this domain.
 
The study by Nocon et al. \cite{Nocon_Kho_Arroyo_2018} explores translating Filipino colloquialisms, such as Conyo and Datkilab, into standardized Filipino, addressing comprehension barriers for non-familiar speakers. Two machine translation (MT) approaches were evaluated: Tensorflow's Sequence-to-Sequence model using Recurrent Neural Networks (RNNs) and Moses' Phrase-based Statistical MT. Moses outperformed Tensorflow on test data due to its handling of phrase combinations and unfamiliar words, while Tensorflow excelled on training data, indicating potential with refinement and more training data. The research underscores the need for robust datasets and highlights the strengths of phrase-based statistical MT in tackling slang translation challenges.

Ibrahim and Mustafa \cite{Abdulstar_Ibrahim_Shareef_Mustafa_2023} developed a system to translate slang into formal language, addressing challenges posed by slang's informality and variability. Using updated datasets of slang words, formal equivalents, and contextual sentences, they fine-tuned pre-trained models from Hugging Face's Transformer library. While the T5-base model showed promise during training, it performed poorly in testing. In contrast, the “facebook/bart-base” model excelled, demonstrating high accuracy and low loss values. The study highlights the importance of fine-tuning and updated datasets for effective slang translation and emphasizes the potential of transformer models like “facebook/bart-base” in bridging informal and formal language gaps. 

\section{LoRA for Fine Tuning}
Low Rank Adaptation, or LoRA, is an efficient Parameter Efficient Fine Tuning (PEFT) method proposed by Hu et al \cite{hu2021loralowrankadaptationlarge}.
It can significantly decrease the required storage for training while producing comparable results and in some cases, even outperforming other adaptation methods.
In addition, it has minimal chance of catastrophic forgetting as the original weights are not being tampered with, unlike other finetuning methods.
These factors make it a suitable option for slang translation as a quick yet accurate solution.
In a study conducted by Zhao et al. \cite{zhao2024loraland310finetuned}, they determined that some LLMs using LoRA for fine tuning can outperform GPT-4, one of the most advanced LLM models currently.
A study by Nguyen et al. \cite{nguyen2023finetuningllama2large} used LoRA in fine tuning a pre-trained Llama 2 7B model for text classification of a dataset that contains slang.
They were able to create a more accurate model compared to models by existing studies at that time. 

\section{Chapter Summary}
This chapter shows how generational differences create communication gaps, especially due to internet slang.
Younger people tend to use slang to express emotions and connect with friends, but this can confuse older generations who aren't as familiar with these terms.
Research shows that as language changes over time, older people are generally less likely to understand the newest internet language.
To bridge this gap, some recent studies have utilized machine learning to translate slang into more standard language.
For instance, Khazeni et al. \cite{Khazeni} used deep learning to translate Persian slang, while Nocon et al. \cite{Nocon_Kho_Arroyo_2018} created a Filipino slang translator using statistical models.
Moreover, Ibrahim and Mustafa \cite{Abdulstar_Ibrahim_Shareef_Mustafa_2023} fine-tuned pre-trained models to learn slang meanings.
One of the promising techniques for this is Low Rank Adaptation (LoRA), which is a fine-tuning method that keeps the original model stable while using less storage.
Studies by Zhao et al. \cite{zhao2024loraland310finetuned} and Nguyen et al. \cite{nguyen2023finetuningllama2large} show that LoRA models are not only efficient but can even outperform advanced models like GPT-4 when it comes to slang translation and text classification. 