%   Filename    : chapter_4.tex 
\chapter{Research Methodology}
This chapter lists and discusses the specific steps and activities that will be performed to accomplish the project. 
The discussion covers the activities from pre-proposal to Final SP Writing.

\section{Research Activities}
\subsection{Creation of the dataset} 
A dataset of sentences containing Generation Z slang and its formal translation or an approximation of will be created. This will involve data scraping of social media posts, use of existing datasets, or any other suitable methods of obtaining data.

For data scraping, we will be using Facebook as the main platform from which data will be collected.
This will involve the use of web scraping scripts using the Facebook API obtained from publicly available scripts on GitHub to search for public posts of people with a birth date between 1997 and 2012. 
The scraped data will then be manually cleaned up and formatted by the researchers.

A complete dataset of sentences containing Generation Z slang is expected at the end of this task. 

\subsection{Identification of potential LLM to be used}
We will be reading upon existing LLM comparison studies to identify potential LLMs to be used for this study.
We will be primarily using studies that used datasets containing slang as they are the most similar to our required dataset but we will still take a look at general translation work using LLMs.

A good potential model is zephyr-7b-beta due to its popularity, more open license, and number of parameters.
Having 7B parameters allows the training of models on a 16GB GPU with a 4-bit quantization.
In addition, zephyr-7b-beta has been proven to be better compared to other LLMs \cite{Zhu_2023} \cite{zhao2024loraland310finetuned}.
These studies have proven that the model is superior to well known models on generative tasks based on automatic and human evaluation.

The model to use should be determined at the end of this task.

\subsection{Lookup for available GPU on demand services} 
Available computing power rental services will be looked up for this study.
As LLM training is a resource-intensive process, it is important to ensure that the necessary computing power is available.
However, this computing power requires expensive equipment that might not see usage after the project is completed.
Thus, it has been decided that it is better to rent the computing power for the duration of the project.

For this project, the computer that will be used for the training must have a minimum of 16GB of vram to load the entire model after quantization and still allow for fine-tuning activities.
The computing power is not as important as it only dictates the length of the training but a faster machine is preferrable to expedite the speed of testing and debugging.

A report on available GPU on demand services will be created using market research and price to computing power ratio.

\subsection{Study on LoRA implementation for LLM}
A thorough study on the implementation of LoRA for fine-tuning will be done.
This includes learning the necessary steps, logic behind the idea, and other necessary information necessary for implementation.
For this step, reading upon guide materials regarding fine-tuning and LoRA as well as existing studies will be done.
We will be primarily using the guide provided by HuggingFace as it is one of the largest repositories for prebuilt LLMs.
In addition, they also provide guides for fine-tuning models for specific purposes and have model specific guides.

\subsection{Preprocessing of data} 
The dataset used for the fine-tuning of the model will be preprocessed to ensure optimal performance of the model.
Data cleaning will be performed to remove errors, inconsistencies, and irrelevant information in the dataset.
This will require removal of non-essential information such as email addresses, URLs, duplicates, etc.
Removal of punctuation and stop words will be performed as these do not contribute to the meaning of the sentence.
The dataset will be checked manually for grammar and accuracy and any errors found will be fixed. 
This is to ensure that the model can focus on learning the patterns between the slang and its formal translation without being affected by noise.

The transformer library by HuggingFace provides a Tokenizer class to automatically convert the cleaned dataset into input for the model.
The model we will be using, which is zephyr-7b-beta model, is a fine-tuned Mistral 7B LLM according to their documentation and is supported by the library.

\subsection{Prototype implementation of LoRA}
A prototype implementation of LoRA will be created using a less demanding model.
This is to avoid incurring costs from constantly retraining the model due to bugs in the code.
The test will also be developed in the same language as the final implementation to avoid any issues with the code translation.
As it is a prototype, it will be used to create a foundation for the complete implementation of LoRA.
This will ensure that during the final implementation, there will be no issues with the code and the model can be fairly evaluated.

For this task, Google Colab will be used as a platform of choice due to the free cloud computing resource and the use of Jupyter notebook.
In addition, Python will be used as the language of choice due to the abundance of available libraries for training LLMs. The transformers library provided by HuggingFace will be used.
This was chosen because of its ease of use with its API, as well as support for popular pretrained models such as LLaMA, BERT, and Mistral.
It is also made by the same company as the zephyr-7b model itself and has even published a guide on fine-tuning the model.

\subsection{Implementation of LoRA on selected model}
A full implementation of LoRA will be done using the previously created prototype as a basis.
We will also be using the entirety of the dataset to train the selected model.
This step will mostly involve tweaking the parameters used to train the selected model and fixing any hidden bugs in the generated results.

\subsection{Implementation of LLM Evaluation Metrics}
A set of automatic evaluation metrics will be used to determine if the fine-tuned model will perform better than the base model.
These metrics will be taken from existing studies on LoRA finetuning and slang translation.
This will serve as the primary measure by which LLMs are compared with each other.
For this purpose, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) will be used to score the generated output compared to ground truth.
Using the LLM as a judge with might also be considered to directly compare the results of the fine-tuned and the base model.
We will be using the Prometheus-eval application from GitHub to assess the model's response using both Prometheus and GPT 4 as judges.
This allows it to mimic human evaluation at a relatively low expense while having reproducible results.

Manual evaluation metrics will also be used to validate the quality of the translation.
This will involve the use of surveys to compare the generated texts between the base model and the fine-tuned model.
This allows for insights that cannot be easily detected by automatic metrics such as sentence quality, coherence, etc.

\subsection{Model Evaluation and Analysis of Results}
The model obtained from previous steps will be evaluated using the evaluation metrics determined from the previous step.
To do this, the testing set split of the dataset will be used as the basis of evaluation.
In addition, descriptive information such as loss function per epoch and perplexity, or the measurement of how well the model predicts text, will be determined.
This information will be used as a supplement to the selected evaluation metrics to determine if the fine-tuned model performed better than the base model.

\subsection{Documentation}
All members are tasked to provide accurate and detailed logs of their activities.
This includes steps on the task they are working on, the status of the work being done, and the time spent on the task.
It will serve both as documentation and as a progress tracker to determine how far the project is from being done.
This will be done every week at the memberâ€™s leisure.


\section{Calendar of Activities}

	Table \ref{tab:timetableactivities} shows a Gantt chart of the activities.  Each bullet represents approximately
	one week's worth of activity.
	
	%
	%  the following commands will be used for filling up the bullets in the Gantt chart
	%
	\newcommand{\weekone}{\textbullet}
	\newcommand{\weektwo}{\textbullet \textbullet}
	\newcommand{\weekthree}{\textbullet \textbullet \textbullet}
	\newcommand{\weekfour}{\textbullet \textbullet \textbullet \textbullet}
	
	
	\begin{table}[ht]  
		\centering
		\caption{Timetable of Activities} \vspace{0.25em}
		\begin{tabular}{|p{2in}|c|c|c|c|c|c|c|c|} \hline
			\centering Activities (2024-2025) 
			& Dec & Jan & Feb & Mar & Apr & May & Jun \\ \hline
			
			Creation of the dataset      
			&\weekone~~~ & & & & & &  \\ \hline
			
			Identification of potential LLM to be used 
			&\weekone~~~ & & & &  &  &  \\ \hline
			
			Lookup on available GPU on demand services     
			&\weekone~~~ & & & &  & &   \\ \hline
			
			Study on LoRA implementation for LLM     
			& ~\weekone & & & &  &  &  \\ \hline
			
			Preprocessing of data      
			& ~\weekthree & & & &  & &  \\ \hline
			
			Prototype implementation of LoRA 
			&~~~\weekone & \weekfour & & &  & &   \\ \hline
			
			Implementation of LoRA on selected model 
			& & &\weektwo~~ & &  &  &  \\ \hline
			
			Implementation of LLM Evaluation Metrics 
			& & &\weektwo  & &  &  &  \\ \hline
			
			Model Evaluation and Analysis of Results 
			& & & & \weekfour & &  &   \\ \hline
			
			Documentation 
			& ~~\weektwo  & \weekfour & \weekfour & \weekfour & \weekfour & &  \\ \hline
			
		\end{tabular}
		\label{tab:timetableactivities}
	\end{table}
