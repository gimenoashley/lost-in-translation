%   Filename    : chapter_4.tex 
\chapter{Research Methodology}
This chapter lists and discusses the specific steps and activities that will be performed to accomplish the project. 
The discussion covers the activities from pre-proposal to Final SP Writing.

\section{Research Activities}
\subsection{Data Gathering} 
A dataset of 1897 sentences containing Generation Z slang and its formal translation was used in this study. 
This dataset was created using several source: data obtained from social media posts and manually translated by the researchers, existing datasets from HuggingFace and Kaggle that was manually verified by the researchers, and machine generated and translated sentences using GPT-4o from OpenAI.

The data obtained from social media posts were from verified users of X whose ages are within the Generation Z, ensuring that the dataset is reflective of the target demographic. The data was then manually cleaned up to remove unnecessary characters such as emojis and fixed issues such as typos. A similar approach was done with existing and machine generated datasets to ensure consistency within the training dataset.

\subsection{Preprocessing of data} 
The dataset used for the fine-tuning of the model was preprocessed to ensure optimal performance of the model.
Unnecessary information such as email addresses and URLs was removed. The dataset is then split into train, evaluation, and test datasets in a  The cleaned up dataset was then tokenized through the Transformers library provided by HuggingFace as the library already has tokenizers available for their pretrained models.
This ensures that the data is formatted properly as required by the model to be used.

Data cleaning will be performed to remove errors, inconsistencies, and irrelevant information in the dataset.
This will require removal of non-essential information such as email addresses, URLs, duplicates, etc.
Removal of punctuation and stop words will be performed as these do not contribute to the meaning of the sentence.
The dataset will be checked manually for grammar and accuracy and any errors found will be fixed. 
This is to ensure that the model can focus on learning the patterns between the slang and its formal translation without being affected by noise.

The transformer library by HuggingFace provides a Tokenizer class to automatically convert the cleaned dataset into input for the model.
The model we will be using, which is zephyr-7b-beta model, is a fine-tuned Mistral 7B LLM according to their documentation and is supported by the library.
The train-test-validation ratio that we will be using is 80-10-10 to ensure no overfitting while still allowing the model to adapt to the pattern of slang.  

\subsection{Identification of potential LLM to be used}
This study opted for the use of a pretrained model and use a fine-tuning approach to modify its behaviour. For this, LoRA was chosen due to the reduced number of parameters modified during training and reducing the overhead of training. The zephyr-7b-beta model by HuggingFace was chosen as the base model to be used as it has been used in several studies and is proven to be better than other LLMs of the same specifications \cite{Zhu_2023} \cite{zhao2024loraland310finetuned}. These studies have compared the model to other models and found that it is superior on generative tasks based on automatic and human evaluation, making it suitable for this study.

\subsection{Prototype model}
A prototype finetuned model was created on Google Colab using existing datasets as the training data.  

A prototype implementation of LoRA will be created using a less demanding model.
This is to avoid incurring costs from constantly retraining the model due to bugs in the code.
The test will also be developed in the same language as the final implementation to avoid any issues with the code translation.
As it is a prototype, it will be used to create a foundation for the complete implementation of LoRA.
This will ensure that during the final implementation, there will be no issues with the code and the model can be fairly evaluated.

For this task, Google Colab will be used as a platform of choice due to the free cloud computing resource, the use of Jupyter notebook, and a computing power-on-demand service. The platform allows us to debug the code for model fine-tuning for free and have the full training by paying for additional resources.
In addition, Python will be used as the language of choice due to the abundance of available libraries for training LLMs. The transformers library provided by HuggingFace will be used.
This was chosen because of its ease of use with its API, as well as support for popular pretrained models such as LLaMA, BERT, and Mistral.
It is also made by the same company as the zephyr-7b model itself and has even published a guide on fine-tuning the model.

\subsection{Implementation of LoRA on selected model}
A full implementation of LoRA will be done using the previously created prototype as a basis.
We will also be using the entirety of the dataset to train the selected model.
This step will mostly involve tweaking the parameters used to train the selected model and fixing any hidden bugs in the generated results.

\subsection{Implementation of LLM Evaluation Metrics}
A set of automatic evaluation metrics will be used to determine if the fine-tuned model will perform better than the base model.
These metrics will be taken from existing studies on LoRA finetuning and slang translation.
This will serve as the primary measure by which LLMs are compared with each other.
For this purpose, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) will be used to score the generated output compared to ground truth.
Using the LLM as a judge with might also be considered to directly compare the results of the fine-tuned and the base model.
We will be using the Prometheus-eval application from GitHub to assess the model's response using both Prometheus and GPT 4 as judges.
This allows it to mimic human evaluation at a relatively low expense while having reproducible results.

Manual evaluation metrics will also be used to validate the quality of the translation.
This will involve the use of surveys to compare the generated texts between the base model and the fine-tuned model.
This allows for insights that cannot be easily detected by automatic metrics such as sentence quality, coherence, etc.

\subsection{Model Evaluation and Analysis of Results}
The model obtained from previous steps will be evaluated using the evaluation metrics determined from the previous step.
To do this, the testing set split of the dataset will be used as the basis of evaluation.
In addition, descriptive information such as loss function per epoch and perplexity, or the measurement of how well the model predicts text, will be determined.
This information will be used as a supplement to the selected evaluation metrics to determine if the fine-tuned model performed better than the base model.

\subsection{Documentation}
All members are tasked to provide accurate and detailed logs of their activities.
This includes steps on the task they are working on, the status of the work being done, and the time spent on the task.
It will serve both as documentation and as a progress tracker to determine how far the project is from being done.
This will be done every week at the memberâ€™s leisure.


\section{Calendar of Activities}

	Table \ref{tab:timetableactivities} shows a Gantt chart of the activities.  Each bullet represents approximately
	one week's worth of activity.
	
	%
	%  the following commands will be used for filling up the bullets in the Gantt chart
	%
	\newcommand{\weekone}{\textbullet}
	\newcommand{\weektwo}{\textbullet \textbullet}
	\newcommand{\weekthree}{\textbullet \textbullet \textbullet}
	\newcommand{\weekfour}{\textbullet \textbullet \textbullet \textbullet}
	
	
	\begin{table}[ht]  
		\centering
		\caption{Timetable of Activities} \vspace{0.25em}
		\begin{tabular}{|p{2in}|c|c|c|c|c|c|c|c|} \hline
			\centering Activities (2024-2025) 
			& Dec & Jan & Feb & Mar & Apr & May & Jun \\ \hline
			
			Creation of the dataset      
			&\weekone~~~ & & & & & &  \\ \hline
			
			Identification of potential LLM to be used 
			&\weekone~~~ & & & &  &  &  \\ \hline
			
			Lookup on available GPU on demand services     
			&\weekone~~~ & & & &  & &   \\ \hline
			
			Study on LoRA implementation for LLM     
			& ~\weekone & & & &  &  &  \\ \hline
			
			Preprocessing of data      
			& ~\weekthree & & & &  & &  \\ \hline
			
			Prototype implementation of LoRA 
			&~~~\weekone & \weekfour & & &  & &   \\ \hline
			
			Implementation of LoRA on selected model 
			& & &\weektwo~~ & &  &  &  \\ \hline
			
			Implementation of LLM Evaluation Metrics 
			& & &\weektwo  & &  &  &  \\ \hline
			
			Model Evaluation and Analysis of Results 
			& & & & \weekfour & &  &   \\ \hline
			
			Documentation 
			& ~~\weektwo  & \weekfour & \weekfour & \weekfour & \weekfour & &  \\ \hline
			
		\end{tabular}
		\label{tab:timetableactivities}
	\end{table}
