%   Filename    : chapter_4.tex 
\chapter{Research Methodology}
This chapter lists and discusses the specific steps and activities that will be performed  to accomplish the project. 
The discussion covers the activities from pre-proposal to Final SP Writing.

\section{Research Activities}
\subsection{Creation of the dataset} 
A dataset of sentences containing Generation Alpha slangs and its formal translation or an approximation of will be created.
This will involve data scraping, use of existing datasets, or any other suitable methods of obtaining data.
This dataset will be used for the training and evaluation of the model.
To ensure it is a high quality dataset, it will be manually checked for accuracy and grammatically correctness.
It will also be checked for any potential biases that may exist in the dataset or the data collection process.

\subsection{Identification of potential LLM to be used}
The researchers will be reading upon existing LLM comparison studies to identify potential LLMs to be used for this study.
They will be primarily using studies that used dataset containing slangs as they are the most similar to the required dataset.

\subsection{Lookup on available GPU on demand services} 
Available computing power rental services will be looked up for this study.
As LLM training are a resource-intensive process, it is important to ensure that the necessary computing power is available.
However, this computing power requires expensive equipment that might not see usage after the project is completed.
Thus, it has been decided that it is better to rent the computing power for the duration of the project.
A report on available GPU on demand services will be created using market research and price to computing power ratio.

\subsection{Study on LoRA implementation for LLM}
A thorough study on the implementation of LoRA for fine-tuning will be done.
This includes learning the necessary steps, logic behind the idea, and other necessary information necessary for implementation.
For this step, reading upon guide materials regarding fine-tuning and LoRA as well as existing studies will be done.
We will be primarily using the guide provided by HuggingFace as it is one of the largest repositories for prebuilt LLMs.
In addition, they also provided guides for fine-tuning models for specific purposes and has model specific guides.

\subsection{Preprocessing of data} 
The dataset used for the fine-tuning of the model will be cleaned up.
This will require removal of non essential information such as email adresses, URLs, etc.
This is to ensure that the model can focus on learning the patterns between the slang and its formal translation without being affected by noise.

\subsection{Prototype implementation of LoRA}
A prototype implementation of LoRA will be created using a less demanding model.
This is to avoid incurring costs from constantly retraining the model due to bugs in the code.
It will be also developed on the same platform as the final implementation to avoid any issues with the code running on different platforms.
As it is a prototype, it will be used to create a foundation for the complete implementation of LoRA.
It will ensure that during the final implementation, there will be no issues with the code and the model can be fairly evaluated.

\subsection{Implementation of LoRA on selected model}
A full implementation of LoRA will be done using the previously created prototype as a basis.
Since it has been proven to work, this step will mostly involve fine-tuning the selected model and fixing any hidden bugs.

\subsection{Implementation on LLM Evaluation Metrics}
A set of evaluation metrics will be used to determine if the fine-tuned model will perform better than the base model.
These metrics will be taken from existing studies on LoRA finetuning and slang translation.
It will serve as the primary measure in which LLMs are compared with from each other.

\subsection{Model Evaluation and Analysis of Results}
The model obtained from previous steps will be evaluated using the evaluation metrics determined from the previous step.
To do this, the testing set split of the dataset will be used as the basis of evaluation.
In addition, descriptive information such as loss function per epoch, accuracy, precision, recall, and F1 score will be determined.
This information will be used as supplement to evaluation metrics to determine if the fine-tuned model will perform better than the base model.

\subsection{Documentation}
All members are tasked to provide accurate and detailed logs of their activities.
This includes steps on the task they are working on, the status of the work being done, and the time spent on the task.
It will serve both as documentation and as a progress tracker to determine how far the project is from being done.
It will be done every week at the memberâ€™s leisure.


\section{Calendar of Activities}

	Table \ref{tab:timetableactivities} shows a Gantt chart of the activities.  Each bullet represents approximately
	one week worth of activity.
	
	%
	%  the following commands will be used for filling up the bullets in the Gantt chart
	%
	\newcommand{\weekone}{\textbullet}
	\newcommand{\weektwo}{\textbullet \textbullet}
	\newcommand{\weekthree}{\textbullet \textbullet \textbullet}
	\newcommand{\weekfour}{\textbullet \textbullet \textbullet \textbullet}
	
	
	\begin{table}[ht]  
		\centering
		\caption{Timetable of Activities} \vspace{0.25em}
		\begin{tabular}{|p{2in}|c|c|c|c|c|c|c|c|} \hline
			\centering Activities (2024-2025) 
			& Nov & Dec & Jan & Feb & Mar & Apr & May \\ \hline
			
			Creation of the dataset      
			&\weekone~~~ & & & & & &  \\ \hline
			
			Identification of potential LLM to be used 
			&\weekone~~~ & & & &  &  &  \\ \hline
			
			Lookup on available GPU on demand services     
			&\weekone~~~ & & & &  & &   \\ \hline
			
			Study on LoRA implementation for LLM     
			& ~\weekone & & & &  &  &  \\ \hline
			
			Preprocessing of data      
			& ~\weekthree & & & &  & &  \\ \hline
			
			Prototype implementation of LoRA 
			&~~~\weekone & \weekfour & & &  & &   \\ \hline
			
			Implementation of LoRA on selected model 
			& & &\weektwo~~ & &  &  &  \\ \hline
			
			Implementation on LLM Evaluation Metrics 
			& & &\weektwo  & &  &  &  \\ \hline
			
			Model Evaluation and Analysis of Results 
			& & & & \weekfour & &  &   \\ \hline
			
			Documentation 
			& ~~\weektwo  & \weekfour & \weekfour & \weekfour & \weekfour & &  \\ \hline
			
		\end{tabular}
		\label{tab:timetableactivities}
	\end{table}