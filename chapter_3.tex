%   Filename    : chapter_4.tex 
\chapter{Research Methodology}
This chapter lists and discusses the specific steps and activities that will be performed  to accomplish the project. 
The discussion covers the activities from pre-proposal to Final SP Writing.

\section{Research Activities}
\subsection{Creation of the dataset} 
A dataset of sentences containing Generation Z slangs and its formal translation or an approximation of will be created.
This will involve data scraping of social media posts, use of existing datasets, or any other suitable methods of obtaining data.

For data scraping, we will be using Facebook as the main platform in which data will be collected from.
This will involve usage of web scraping scripts using Facebook API to search for public posts of people with a birth date between 1997 and 2012.
The scraped data will then be manually cleaned up and formatted by the researchers.
This dataset will be used for the training and evaluation of the model.
To ensure it is a high quality dataset, it will be manually checked for accuracy and grammatically correctness.
It will also be checked for any potential biases that may exist in the dataset or the data collection process.

A complete dataset of sentences containing Generation Alpha slangs is expected at the end of this task. 

\subsection{Identification of potential LLM to be used}
We will be reading upon existing LLM comparison studies to identify potential LLMs to be used for this study.
We will be primarily using studies that used dataset containing slangs as they are the most similar to our required dataset.
A good potential model is zephyr-7b-beta due to its popularity, more open license, and number of parameters.
Having 7B parameters allows the training of models on a 16GB GPU with a 4-bit quantization.

A model to use should be determined at the end of this task.

\subsection{Lookup on available GPU on demand services} 
Available computing power rental services will be looked up for this study.
As LLM training are a resource-intensive process, it is important to ensure that the necessary computing power is available.
However, this computing power requires expensive equipment that might not see usage after the project is completed.
Thus, it has been decided that it is better to rent the computing power for the duration of the project.
A report on available GPU on demand services will be created using market research and price to computing power ratio.

\subsection{Study on LoRA implementation for LLM}
A thorough study on the implementation of LoRA for fine-tuning will be done.
This includes learning the necessary steps, logic behind the idea, and other necessary information necessary for implementation.
For this step, reading upon guide materials regarding fine-tuning and LoRA as well as existing studies will be done.
We will be primarily using the guide provided by HuggingFace as it is one of the largest repositories for prebuilt LLMs.
In addition, they also provided guides for fine-tuning models for specific purposes and has model specific guides.

\subsection{Preprocessing of data} 
The dataset used for the fine-tuning of the model will be cleaned up.
This will require removal of non essential information such as email adresses, URLs, etc.
This is to ensure that the model can focus on learning the patterns between the slang and its formal translation without being affected by noise.

A clean dataset ready for tokenization is expected at the end of this task.

\subsection{Prototype implementation of LoRA}
A prototype implementation of LoRA will be created using a less demanding model.
This is to avoid incurring costs from constantly retraining the model due to bugs in the code.
It will be also developed on the same language as the final implementation to avoid any issues with the code translation.
As it is a prototype, it will be used to create a foundation for the complete implementation of LoRA.
It will ensure that during the final implementation, there will be no issues with the code and the model can be fairly evaluated.

For this task, Google Colab will be used as a platform of choice due to the free cloud computing resource and the use of Jupyter notebook.
In addition, Python will be used as the language of choice to the abundance of available libraries for training LLMs.

\subsection{Implementation of LoRA on selected model}
A full implementation of LoRA will be done using the previously created prototype as a basis.
This step will mostly involve tweaking the parameters used to train the selected model and fixing any hidden bugs in the generated results.

\subsection{Implementation on LLM Evaluation Metrics}
A set of evaluation metrics will be used to determine if the fine-tuned model will perform better than the base model.
These metrics will be taken from existing studies on LoRA finetuning and slang translation.
It will serve as the primary measure in which LLMs are compared with from each other.
For this purpose, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) will be used to score the generated output compared to ground truth.
The use of LLM as a judge might also be considered to directly compare the results of the fine-tuned and the base model.

\subsection{Model Evaluation and Analysis of Results}
The model obtained from previous steps will be evaluated using the evaluation metrics determined from the previous step.
To do this, the testing set split of the dataset will be used as the basis of evaluation.
In addition, descriptive information such as loss function per epoch and perplexity will be determined.
This information will be used as supplement to evaluation metrics to determine if the fine-tuned model performed better than the base model.

\subsection{Documentation}
All members are tasked to provide accurate and detailed logs of their activities.
This includes steps on the task they are working on, the status of the work being done, and the time spent on the task.
It will serve both as documentation and as a progress tracker to determine how far the project is from being done.
It will be done every week at the memberâ€™s leisure.


\section{Calendar of Activities}

	Table \ref{tab:timetableactivities} shows a Gantt chart of the activities.  Each bullet represents approximately
	one week worth of activity.
	
	%
	%  the following commands will be used for filling up the bullets in the Gantt chart
	%
	\newcommand{\weekone}{\textbullet}
	\newcommand{\weektwo}{\textbullet \textbullet}
	\newcommand{\weekthree}{\textbullet \textbullet \textbullet}
	\newcommand{\weekfour}{\textbullet \textbullet \textbullet \textbullet}
	
	
	\begin{table}[ht]  
		\centering
		\caption{Timetable of Activities} \vspace{0.25em}
		\begin{tabular}{|p{2in}|c|c|c|c|c|c|c|c|} \hline
			\centering Activities (2024-2025) 
			& Dec & Jan & Feb & Mar & Apr & May & Jun \\ \hline
			
			Creation of the dataset      
			&\weekone~~~ & & & & & &  \\ \hline
			
			Identification of potential LLM to be used 
			&\weekone~~~ & & & &  &  &  \\ \hline
			
			Lookup on available GPU on demand services     
			&\weekone~~~ & & & &  & &   \\ \hline
			
			Study on LoRA implementation for LLM     
			& ~\weekone & & & &  &  &  \\ \hline
			
			Preprocessing of data      
			& ~\weekthree & & & &  & &  \\ \hline
			
			Prototype implementation of LoRA 
			&~~~\weekone & \weekfour & & &  & &   \\ \hline
			
			Implementation of LoRA on selected model 
			& & &\weektwo~~ & &  &  &  \\ \hline
			
			Implementation on LLM Evaluation Metrics 
			& & &\weektwo  & &  &  &  \\ \hline
			
			Model Evaluation and Analysis of Results 
			& & & & \weekfour & &  &   \\ \hline
			
			Documentation 
			& ~~\weektwo  & \weekfour & \weekfour & \weekfour & \weekfour & &  \\ \hline
			
		\end{tabular}
		\label{tab:timetableactivities}
	\end{table}